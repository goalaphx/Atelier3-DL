{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a476ef1b-5b63-4c4a-82c1-d2feb8831c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped and saved dataset from Al Jazeera and Al Arabiya.\n",
      "                                                text  score\n",
      "0  كشفت بيانات ملاحية أن سفينة متجهة لإسرائيل رست...    0.3\n",
      "1  خرجت مظاهرات حاشدة في عدد من العواصم والمدن حو...    1.6\n",
      "2  شهدت منصات التواصل الاجتماعي في مصر حالة من ال...    9.3\n",
      "3  طوت الولايات المتحدة الأميركية وإيران صفحة الج...    2.1\n",
      "4  عادت طائرة بوينغ 737 ماكس كانت مخصصة لشركة طير...    7.0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def scrape_site(url, p_tag='p', min_length=30):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    paragraphs = soup.find_all(p_tag)\n",
    "    texts = [p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > min_length]\n",
    "    return texts\n",
    "\n",
    "# Scrape Al Jazeera\n",
    "aljazeera_url = \"https://www.aljazeera.net/news\"\n",
    "texts_jazeera = scrape_site(aljazeera_url)\n",
    "\n",
    "# Scrape Al Arabiya\n",
    "alarabiya_url = \"https://www.alarabiya.net/arab-and-world\"\n",
    "texts_arabiya = scrape_site(alarabiya_url)\n",
    "\n",
    "# Combine and assign random scores\n",
    "combined_texts = texts_jazeera[:25] + texts_arabiya[:25]\n",
    "data = [{'text': text, 'score': round(random.uniform(0, 10), 1)} for text in combined_texts]\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('arabic_combined_dataset.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"✅ Scraped and saved dataset from Al Jazeera and Al Arabiya.\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6488692-f3cc-4825-bccd-02c2f0d0d161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing complete. Sample:\n",
      "                                          clean_text  score\n",
      "0  كشفت بيانات ملاحية أن سفينة متجهة لإسرائيل رست...    0.3\n",
      "1  خرجت مظاهرات حاشدة في عدد من العواصم والمدن حو...    1.6\n",
      "2  شهدت منصات التواصل الاجتماعي في مصر حالة من ال...    9.3\n",
      "3  طوت الولايات المتحدة الأميركية وإيران صفحة الج...    2.1\n",
      "4  عادت طائرة بوينغ ماكس كانت مخصصة لشركة طيران ص...    7.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "# ✅ Download Arabic stopwords from GitHub\n",
    "stopwords_url = 'https://raw.githubusercontent.com/mohataher/arabicstopwords/master/list.txt'\n",
    "response = requests.get(stopwords_url)\n",
    "arabic_stopwords = set(response.text.splitlines())\n",
    "\n",
    "# ✅ Load the dataset\n",
    "df = pd.read_csv(\"arabic_combined_dataset.csv\")\n",
    "\n",
    "# ✅ Clean and preprocess\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)  # Keep Arabic characters only\n",
    "    text = dediac_ar(text)  # Remove diacritics\n",
    "    tokens = simple_word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in arabic_stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['clean_text'] = df['text'].apply(preprocess)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(\"✅ Preprocessing complete. Sample:\")\n",
    "print(df[['clean_text', 'score']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e03215-799e-4ee3-99af-c3664f5565a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Training RNN model...\n",
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 33.2224 - mae: 4.8422 - val_loss: 5.4661 - val_mae: 2.1873\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - loss: 28.3096 - mae: 4.3677 - val_loss: 5.0442 - val_mae: 2.1152\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - loss: 23.7490 - mae: 3.9456 - val_loss: 3.9964 - val_mae: 1.8758\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - loss: 19.5147 - mae: 3.4957 - val_loss: 2.4516 - val_mae: 1.3917\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - loss: 16.3631 - mae: 3.2190 - val_loss: 1.3487 - val_mae: 0.8923\n",
      "\n",
      "🔧 Training BiRNN model...\n",
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 31.2876 - mae: 4.6800 - val_loss: 3.2054 - val_mae: 1.5612\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 24.5906 - mae: 4.0397 - val_loss: 2.2638 - val_mae: 1.2338\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 20.4946 - mae: 3.6100 - val_loss: 1.4862 - val_mae: 0.8907\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 17.4816 - mae: 3.3120 - val_loss: 1.0755 - val_mae: 0.7946\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 15.0823 - mae: 3.0693 - val_loss: 0.8784 - val_mae: 0.7217\n",
      "\n",
      "🔧 Training GRU model...\n",
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 34.1358 - mae: 4.9557 - val_loss: 6.1761 - val_mae: 2.3246\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 33.7779 - mae: 4.9218 - val_loss: 6.0996 - val_mae: 2.3078\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 33.4180 - mae: 4.8873 - val_loss: 6.0210 - val_mae: 2.2903\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 33.0458 - mae: 4.8510 - val_loss: 5.9390 - val_mae: 2.2721\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - loss: 32.6524 - mae: 4.8121 - val_loss: 5.8521 - val_mae: 2.2526\n",
      "\n",
      "🔧 Training LSTM model...\n",
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 34.2597 - mae: 4.9691 - val_loss: 6.1165 - val_mae: 2.3128\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 33.9502 - mae: 4.9394 - val_loss: 6.0465 - val_mae: 2.2976\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - loss: 33.6316 - mae: 4.9086 - val_loss: 5.9663 - val_mae: 2.2801\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 33.2783 - mae: 4.8742 - val_loss: 5.8713 - val_mae: 2.2593\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - loss: 32.8694 - mae: 4.8340 - val_loss: 5.7548 - val_mae: 2.2337\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize and pad\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['clean_text'])\n",
    "X = tokenizer.texts_to_sequences(df['clean_text'])\n",
    "X = pad_sequences(X, maxlen=100)\n",
    "y = df['score'].values\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build model function\n",
    "def build_model(model_type='RNN'):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
    "\n",
    "    if model_type == 'RNN':\n",
    "        model.add(tf.keras.layers.SimpleRNN(64))\n",
    "    elif model_type == 'BiRNN':\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(64)))\n",
    "    elif model_type == 'GRU':\n",
    "        model.add(tf.keras.layers.GRU(64))\n",
    "    elif model_type == 'LSTM':\n",
    "        model.add(tf.keras.layers.LSTM(64))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1))  # Regression\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Train all models\n",
    "models = ['RNN', 'BiRNN', 'GRU', 'LSTM']\n",
    "trained_models = {}\n",
    "\n",
    "for name in models:\n",
    "    print(f\"\\n🔧 Training {name} model...\")\n",
    "    model = build_model(name)\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=16, validation_split=0.2)\n",
    "    trained_models[name] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4268c925-f0b7-48bf-bd07-20fe2e958a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "\n",
      "📊 Evaluation for RNN model:\n",
      "MAE: 1.58\n",
      "MSE: 3.01\n",
      "R² Score: -0.29\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step\n",
      "\n",
      "📊 Evaluation for BiRNN model:\n",
      "MAE: 1.58\n",
      "MSE: 2.52\n",
      "R² Score: -0.08\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step\n",
      "\n",
      "📊 Evaluation for GRU model:\n",
      "MAE: 2.33\n",
      "MSE: 7.68\n",
      "R² Score: -2.29\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step\n",
      "\n",
      "📊 Evaluation for LSTM model:\n",
      "MAE: 2.35\n",
      "MSE: 7.85\n",
      "R² Score: -2.36\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Evaluate all models\n",
    "for name, model in trained_models.items():\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    print(f\"\\n📊 Evaluation for {name} model:\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
    "    print(f\"MSE: {mean_squared_error(y_test, y_pred):.2f}\")\n",
    "    print(f\"R² Score: {r2_score(y_test, y_pred):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
