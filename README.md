# ğŸ“Š Arabic Text Regression Using Deep Learning

This project explores **Arabic Natural Language Processing (NLP)** by predicting continuous values (scores) from Arabic text using machine learning and deep learning models. The full pipeline includes:

- ğŸŒ Data scraping
- ğŸ§¼ Text preprocessing
- ğŸ§  Deep learning modeling
- ğŸ“ˆ Model evaluation

---

## 1. ğŸŒ Data Collection

Arabic news headlines were scraped from two major websites:

- ğŸ“° Al Jazeera (https://www.aljazeera.net)
- ğŸ“° Al Arabiya (https://www.alarabiya.net)

Each articleâ€™s title was assigned a random score between 0 and 10 to simulate a regression problem.

**Example of scraped data:**

| Text | Score |
|------|-------|
| ÙƒØ´ÙØª Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù„Ø§Ø­ÙŠØ© Ø£Ù† Ø³ÙÙŠÙ†Ø© Ù…ØªØ¬Ù‡Ø© Ù„Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ Ø±Ø³Øª... | 0.3 |
| Ø®Ø±Ø¬Øª Ù…Ø¸Ø§Ù‡Ø±Ø§Øª Ø­Ø§Ø´Ø¯Ø© ÙÙŠ Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø¹ÙˆØ§ØµÙ… ÙˆØ§Ù„Ù…Ø¯Ù† Ø­Ùˆ... | 1.6 |
| Ø´Ù‡Ø¯Øª Ù…Ù†ØµØ§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ ÙÙŠ Ù…ØµØ± Ø­Ø§Ù„Ø© Ù…Ù† Ø§Ù„... | 9.3 |

---

## 2. ğŸ§¼ Text Preprocessing

Arabic text was cleaned using the CAMeL Tools library. The steps included:

- Removing diacritics
- Tokenization
- Removing stopwords (from a custom Arabic stopword list)
- Cleaning punctuation and special characters

**Example of cleaned text:**

| Cleaned Text | Score |
|--------------|--------|
| ÙƒØ´ÙØª Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù„Ø§Ø­ÙŠØ© Ø£Ù† Ø³ÙÙŠÙ†Ø© Ù…ØªØ¬Ù‡Ø© Ù„Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ Ø±Ø³Øª... | 0.3 |

This improves model accuracy by reducing noise in the input.

---

## 3. ğŸ§  Model Training

We trained and compared the following deep learning models using Keras:

- ğŸ” RNN (Recurrent Neural Network)
- ğŸ” BiRNN (Bidirectional RNN)
- ğŸ” GRU (Gated Recurrent Unit)
- ğŸ” LSTM (Long Short-Term Memory)

All models used:

- Arabic text â†’ tokenized + padded sequences
- Embedding layer
- Output: Regression (linear activation)
- Loss: Mean Squared Error (MSE)
- Optimizer: Adam
- Epochs: 5

Example (BiRNN - last epoch):

Epoch 5/5 loss: 15.08 - mae: 3.06 - val_loss: 0.87 - val_mae: 0.72

---

## 4. ğŸ“ˆ Model Evaluation

Models were evaluated using:

- **MAE**: Mean Absolute Error
- **MSE**: Mean Squared Error
- **RÂ²**: Coefficient of Determination

| Model   | MAE  | MSE  | RÂ² Score |
|---------|------|------|----------|
| RNN     | 1.58 | 3.01 | -0.29    |
| **BiRNN** | **1.58** | **2.52** | **-0.08** |
| GRU     | 2.33 | 7.68 | -2.29    |
| LSTM    | 2.35 | 7.85 | -2.36    |

**ğŸ§  Best Model:** BiRNN gave the lowest error and best RÂ² score.

---

## ğŸ§° Tools & Libraries Used

- Python 3.x
- camel-tools
- BeautifulSoup, requests
- TensorFlow, Keras
- scikit-learn
- pandas, matplotlib

---

## ğŸ“Œ Conclusion

This project demonstrates end-to-end Arabic text regression using deep learning. Despite the limited dataset, the BiRNN model showed promising results. Preprocessing Arabic correctly is essential to achieve good performance.

---

## ğŸš€ Future Improvements

- Use a larger real-world dataset with labeled scores
- Fine-tune Arabic pretrained models (e.g., AraBERT)
- Try transformers for improved results







---

## ğŸ§  GPT-2 Fine-Tuning on Arabic Text (Colab)

This project demonstrates how to fine-tune the pre-trained [GPT-2 language model](https://huggingface.co/gpt2) using Arabic text data from the [OSCAR](https://huggingface.co/datasets/oscar) dataset.

### ğŸš€ What this notebook does:

- Loads a small Arabic text dataset (1000 samples) for fast training
- Tokenizes the text using the GPT-2 tokenizer
- Fine-tunes the GPT-2 model for 1 epoch using Hugging Face's `Trainer`
- Generates new Arabic text from a custom prompt

---

### ğŸ“š Dataset

- **Source**: [OSCAR](https://huggingface.co/datasets/oscar)
- **Subset**: `unshuffled_deduplicated_ar`
- **Sample size**: 1000 Arabic text examples
- The data is filtered to remove short or empty lines for quality

---

### ğŸ› ï¸ Libraries Used

- [Transformers](https://huggingface.co/docs/transformers/index) by Hugging Face
- [Datasets](https://huggingface.co/docs/datasets/index) by Hugging Face
- PyTorch (via `transformers` backend)
- Google Colab as the development environment

---

### ğŸ“¦ How to Run

1. Open the notebook in Google Colab
2. Install required libraries:
   ```bash
   !pip install transformers datasets
   ```
3. Run each cell to:
   - Load and tokenize the dataset
   - Fine-tune the model on Arabic text
   - Generate new Arabic sentences from a custom prompt

---

### âœ¨ Example Output

Prompt:
```
Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ
```

Generated text:
```
Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ Ù…Ù† Ø£Ù‡Ù… Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªÙŠ ØªØ³Ø§Ø¹Ø¯ Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø§Ù„Ø®Ø¯Ù…Ø§Øª ÙˆØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø­ÙŠØ§Ø© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ø¯ÙŠØ«...
```

---

### ğŸ§  Notes

- This is a lightweight demo intended for educational purposes.
- GPT-2 was originally trained on English text. For serious Arabic NLP applications, consider using models pre-trained on Arabic like [AraGPT2](https://huggingface.co/aubmindlab/aragpt2-mega).

---

### ğŸ“ Output

- Fine-tuned model saved to: `./gpt2_arabic_demo/`
- Can be reused for further training or inference

---


